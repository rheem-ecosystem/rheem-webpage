+++
title = "A Cost-based Optimizer for Gradient Descent Optimization"
conference = "SIGMOD"
year = "2017"
link = "http://sigmod2017.org/"
city = "Chicago"
country =  "USA"
type = ""
date = "2017-02-01T13:07:31+02:00"
author = "Jorge"
imagen = "/img/screenshot/ml4all.png"
paper = "/pdf/paper/ml4all.pdf"
tags = ["publication"]
categories = ["publication"]
+++
As the use of machine learning (ML) permeates into diverse application domains, there is an urgent need to support a declarative framework for ML. Ideally, a user will specify an ML task in a high-level and easy-to-use language and the framework will invoke the appropriate algorithms and sys- tem configurations to execute it. An important observation towards designing such a framework is that many ML tasks can be expressed as mathematical optimization problems, which take a specific form. Furthermore, these optimiza- tion problems can be efficiently solved using variations of the gradient descent (GD) algorithm. Thus, to decouple a user specification of an ML task from its execution, a key component is a GD optimizer. We propose a cost-based GD optimizer that selects the best GD plan for a given ML task. To build our optimizer, we introduce a set of abstract oper- ators for expressing GD algorithms and propose a novel ap- proach to estimate the number of iterations a GD algorithm requires to converge. Extensive experiments on real and syn- thetic datasets show that our optimizer not only chooses the best GD plan but also allows for optimizations that achieve orders of magnitude performance speed-up.